{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from darts import TimeSeries\n",
    "from utils import build_dataset_from_df, get_likelihood, get_optim_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"../experiments/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented.pkl\")\n",
    "base_df = pd.read_pickle(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def calculate_mses(base_df):\n",
    "    losses = []\n",
    "    rnn_losses = []\n",
    "    deepar_losses = []\n",
    "    for idx, target in enumerate(sorted(base_df.hash_func.unique())):\n",
    "        df = base_df[base_df.hash_func == target]\n",
    "\n",
    "        model_dir = Path(f\"../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i={idx}\")\n",
    "        model_path = list(model_dir.glob(\"**/nhits/**/model.pt\"))[0]\n",
    "        rnn_model_path = list(model_dir.glob(\"**/rnn/**/model.pt\"))[0]\n",
    "        deepar_model_path = list(model_dir.glob(\"**/deepar/**/model.pt\"))[0]\n",
    "        model_path_str = str(model_path)\n",
    "\n",
    "        begin = model_path_str.find(\"clen=\") + 5\n",
    "        end = model_path_str.find(\"_\", begin)\n",
    "        clen = int(model_path_str[begin:end])\n",
    "        begin = end+6\n",
    "        end = model_path_str.find(\"/\", begin)\n",
    "        plen = int(model_path_str[begin:end])\n",
    "        print(clen, plen)\n",
    "\n",
    "        print(\"Loading model from %s\" % model_path)\n",
    "\n",
    "        scaler_path = model_path.with_name(\"scaler.pt\")\n",
    "        scaler = torch.load(scaler_path)\n",
    "\n",
    "        map_location = torch.device(\"cpu\")\n",
    "        with open(model_path, \"rb\") as fin:\n",
    "            model = torch.load(fin, map_location=map_location)\n",
    "\n",
    "        ckpt_path = model_path.with_name(model_path.name + \".ckpt\")\n",
    "        if ckpt_path.exists():\n",
    "            model.model = model.model.__class__.load_from_checkpoint(ckpt_path)\n",
    "            model.trainer = None\n",
    "        model.trainer_params = {\n",
    "            \"enable_progress_bar\": False,\n",
    "            \"logger\": False,\n",
    "        }\n",
    "\n",
    "        print(\"Loading rnn model from %s\" % rnn_model_path)\n",
    "        with open(rnn_model_path, \"rb\") as fin:\n",
    "            rnn_model = torch.load(fin, map_location=map_location)\n",
    "\n",
    "        ckpt_path = rnn_model_path.with_name(rnn_model_path.name + \".ckpt\")\n",
    "        if ckpt_path.exists():\n",
    "            rnn_model.model = rnn_model.model.__class__.load_from_checkpoint(ckpt_path)\n",
    "            rnn_model.trainer = None\n",
    "        rnn_model.trainer_params = {\n",
    "            \"enable_progress_bar\": False,\n",
    "            \"logger\": False,\n",
    "        }\n",
    "\n",
    "        print(\"Loading deepar model from %s\" % deepar_model_path)\n",
    "        with open(deepar_model_path, \"rb\") as fin:\n",
    "            deepar_model = torch.load(fin, map_location=map_location)\n",
    "\n",
    "        ckpt_path = deepar_model_path.with_name(deepar_model_path.name + \".ckpt\")\n",
    "        if ckpt_path.exists():\n",
    "            deepar_model.model = deepar_model.model.__class__.load_from_checkpoint(ckpt_path)\n",
    "            deepar_model.trainer = None\n",
    "        deepar_model.trainer_params = {\n",
    "            \"enable_progress_bar\": False,\n",
    "            \"logger\": False,\n",
    "        }\n",
    "\n",
    "        has_sample_col = \"sample\" in df.columns\n",
    "        dataset = build_dataset_from_df(df, check_validity=(not has_sample_col))\n",
    "        # dataset_ts is ordered by 'group'\n",
    "        dataset_ts = TimeSeries.from_group_dataframe(\n",
    "            dataset, group_cols=\"group\", time_col=\"time_idx\", value_cols=\"value\")\n",
    "\n",
    "        if has_sample_col:\n",
    "            # regenerate dataset for each sample by using scaler for each function\n",
    "            raw_data = []\n",
    "            for sample_idx in sorted(df[\"sample\"].unique()):\n",
    "                dataset = build_dataset_from_df(df[df[\"sample\"] == sample_idx])\n",
    "                dataset_ts = TimeSeries.from_group_dataframe(\n",
    "                    dataset, group_cols=\"group\", time_col=\"time_idx\", value_cols=\"value\")\n",
    "                # scaler order will be the same as the dataset_ts order\n",
    "                raw_data.extend(dataset_ts)\n",
    "\n",
    "        count_len = int(df.counts.agg(lambda x: x.size).max())\n",
    "        print(f\"max count len: {count_len}\", flush=True)\n",
    "\n",
    "        # splits = [ts_item.split_after(count_len-1) for ts_item in scaled]\n",
    "        # val, train = zip(*splits)\n",
    "        # train = [ts.shift(-count_len) for ts in train]\n",
    "        splits = [ts_item.split_after(len(ts_item) - count_len - 1) for ts_item in raw_data]\n",
    "        train, val = zip(*splits)\n",
    "\n",
    "        scaled = [scaler.transform(v) for v in val]\n",
    "        val_dataset = model._build_train_dataset(scaled, None, None, None)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, drop_last=False, shuffle=False, collate_fn=model._batch_collate_fn)\n",
    "        rnn_model.model.set_predict_parameters(n=plen, num_samples=1, roll_size=1, batch_size=32, n_jobs=1)\n",
    "        deepar_model.model.set_predict_parameters(n=plen, num_samples=30, roll_size=1, batch_size=32, n_jobs=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_batch in val_loader:\n",
    "                x, _, _, y = input_batch\n",
    "                # print(x.shape)\n",
    "                y2 = torch.FloatTensor(scaler._fitted_params[0].inverse_transform(y[:, :, 0]))\n",
    "                pred = model.model((x, None))\n",
    "                pred2 = torch.FloatTensor(scaler._fitted_params[0].inverse_transform(pred[:, :, 0, 0]))\n",
    "                losses.append(loss_fn(pred2, y2).item())\n",
    "                rnn_pred = rnn_model.model._get_batch_prediction(plen, input_batch, 1)\n",
    "                rnn_pred2 = torch.FloatTensor(scaler._fitted_params[0].inverse_transform(rnn_pred[:, :, 0]))\n",
    "                rnn_losses.append(loss_fn(rnn_pred2, y2).item())\n",
    "                deepar_pred = []\n",
    "                for _ in range(deepar_model.model.pred_num_samples):\n",
    "                    deepar_pred.append(deepar_model.model._get_batch_prediction(plen, input_batch, 1))\n",
    "                deepar_pred = torch.stack(deepar_pred).mean(axis=0)\n",
    "                deepar_pred2 = torch.FloatTensor(scaler._fitted_params[0].inverse_transform(deepar_pred[:, :, 0]))\n",
    "                deepar_losses.append(loss_fn(deepar_pred2, y2).item())\n",
    "                # break\n",
    "\n",
    "        # break\n",
    "\n",
    "    return losses, rnn_losses, deepar_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pred.shape)\n",
    "# print(rnn_pred.shape)\n",
    "# print(deepar_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=0/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=0/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=0/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=1/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=1/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=1/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "10 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=2/seed=42/darts/clen=10_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=2/seed=42/darts/clen=10_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=2/seed=42/darts/clen=10_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=3/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=3/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=3/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=4/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=4/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=4/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=5/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=5/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=5/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=6/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=6/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=6/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=7/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=7/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=7/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "10 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=8/seed=42/darts/clen=10_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=8/seed=42/darts/clen=10_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=8/seed=42/darts/clen=10_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n",
      "15 7\n",
      "Loading model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=9/seed=42/darts/clen=15_plen=7/nhits/bs=32_lr=0.001/b=1_s=3_l=2_lw=512_do=0.1/model.pt\n",
      "Loading rnn model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=9/seed=42/darts/clen=15_plen=7/rnn/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1/model.pt\n",
      "Loading deepar model from ../results_local/pred/top9_twitter_1_1600_avgproc_min_int5m_reduced_6hr_augmented_i=9/seed=42/darts/clen=15_plen=7/deepar/bs=32_lr=0.001/LSTM/l=2_lw=512_do=0.1_gaussian/model.pt\n",
      "max count len: 360\n"
     ]
    }
   ],
   "source": [
    "nhits_losses, rnn_losses, deepar_losses = calculate_mses(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116.24016873484885"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(nhits_losses))\n",
    "np.sqrt(np.mean(nhits_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123.93639708611852"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(rnn_losses))\n",
    "np.sqrt(np.mean(rnn_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "122.37697701215882"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(deepar_losses))\n",
    "np.sqrt(np.mean(deepar_losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k8s-ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
